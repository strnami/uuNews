const { BaseError, Config } = require("uu_appg01_core-utils");
const { LoggerFactory } = require("uu_appg01_core-logging");
const logger = LoggerFactory.get("UuApp.Workspace.Models.SysDumpRestoreModel");
const { Validator } = require("uu_appg01_core-validation");
const { ValidationHelper } = require("uu_appg01_core-appserver");
const SysAppRuntimeModeModel = require("./sys-app-runtime-mode-model");
const SysAppWorkspaceModel = require("./sys-app-workspace-model");
const SysDataStoreStatsModel = require("./sys-data-store-stats-model");
const SysAppWorkspaceConfigModel = require("./sys-app-workspace-config-model");
const SysDumpRestoreMongo = require("../daos/sys-dump-restore-mongo");
const { Sys } = require("uu_appg01_core-appserver");

const sizeOf = require("object-sizeof");
const pump = require("pump");

const { DaoFactory, ObjectStoreError } = require("uu_appg01_objectstore");
const s = require("stream");
const { Transform } = s;
const LineByLineReader = require("line-by-line");
const DataStoreDumpDao = require("../daos/sys-dump-restore-mongo");
const { EJSON, Binary } = require("bson");
const JSZIP = require("jszip");
const unzip = require("unzip-stream");
const DeepMerge = require("deepmerge");

const ImportMetadata = require("../helpers/import-metadata");

const MAXIMUM_DUMP_SIZE = Config.get("uu_app_workspace_maximum_dump_size") || 1024 * 1024 * 1024;
const Errors = require("../errors/sys-dump-restore-errors");
const WorkspaceError = require("../errors/workspace-error");
const STATUS_FILE = "status.json";
const WARNINGS = {
  importUnsupportedKeys: `${Errors.SysImport.UC_CODE}unsupportedKeys`,
  importIndexImportFailed: `${Errors.SysImport.UC_CODE}indexImportFailed`,
  importConflictingProfiles: `${Errors.SysImport.UC_CODE}conflictingProfiles`,
  listMappingsUnsupportedKeys: `${Errors.SysListImportMappings.UC_CODE}unsupportedKeys`
};
const QUEUE_LIMIT = 32000; //32KB
const PRIVILEGED_MODE = "privileged";
const IMPORT_MAPPING_SCHEMA = "sysImportMapping";

class SysDumpRestoreModel {
  constructor() {
    this.validator = Validator.load();
    this.importMappingDao = DaoFactory.getDao(IMPORT_MAPPING_SCHEMA);
  }

  async import(awid, dtoIn) {
    // hds 1
    let appWorkspace;
    try {
      appWorkspace = await SysAppWorkspaceModel.get(awid);
    } catch (e) {
      if (e instanceof WorkspaceError) { // A1
        throw new Errors.SysImport.SysAppWorkspaceDoesNotExist({}, { awid });
      }
      throw e;
    }

    // hds 2
    if (!appWorkspace.runtimeMode || appWorkspace.runtimeMode.mode !== PRIVILEGED_MODE) {
      throw new Errors.SysImport.AwidNotInPrivilegedMode({}); // A2
    }

    // hds 3, 3.1, 3.2, 3.3, A3, A4
    let validationResult = this.validator.validate("sysImportDtoInType", dtoIn);
    let uuAppErrorMap = ValidationHelper.processValidationResult(
      dtoIn,
      validationResult,
      WARNINGS.importUnsupportedKeys,
      Errors.SysImport.InvalidDtoIn
    );

    // hds 4
    await new Promise((resolve, reject) => {
      let unzipStream = pump(dtoIn.data, unzip.Parse());
      let entryPromise; // indicates whether last entry was processed so dtoOut returns after restoring all data
      let deletedBinaryRegister = [];
      let fileIdMapping = {};

      unzipStream.on("entry", async entry => {
        entryPromise = new Promise(async resolve => {
          try {
            unzipStream.pause();
            entryPromise = await this._importSchema(entry, uuAppErrorMap, awid, appWorkspace, deletedBinaryRegister, fileIdMapping);
            unzipStream.resume();
            resolve();
          } catch (e) {
            unzipStream.destroy(e);
          }
        });
      });

      unzipStream.on("error", e => {
        if (e instanceof WorkspaceError) {
          reject(e);
        } else {
          reject(new Errors.SysImport.ImportFailed({ uuAppErrorMap }, { cause: e.message }, e));
        }
      });

      unzipStream.on("end", async () => {
        if (entryPromise) {
          await entryPromise;
        }
        resolve();
      });
    });

    //clear caches
    SysAppWorkspaceConfigModel.clearCache();
    SysAppWorkspaceModel.clearCache();

    return { uuAppErrorMap };
  }

  async deleteImportMappings(awid) {
    // hds 1
    await this.importMappingDao.delete(awid);

    // hds 2
    return { uuAppErrorMap: {} };
  }

  async listImportMappings(awid, dtoIn) {
    // hds 1, A1, A2
    let validationResult = this.validator.validate("sysListImportMappingsDtoInType", dtoIn);
    let uuAppErrorMap = ValidationHelper.processValidationResult(
      dtoIn,
      validationResult,
      WARNINGS.listMappingsUnsupportedKeys,
      Errors.SysListImportMappings.InvalidDtoIn
    );

    // hds 2
    let list;
    if (dtoIn.origIdList) {
      // hds 2.2
      list = await this.importMappingDao.listByOrigIdList(awid, dtoIn.origIdList, dtoIn.pageInfo || {});
    } else {
      // hds 2.1
      list = await this.importMappingDao.list(awid, dtoIn.pageInfo || {});
    }

    // hds 3
    return { ...list, uuAppErrorMap };
  }

  /**
   * Return dump of database as a stream
   * @param dtoIn
   * @param requirePrivilegedMode
   * @returns {Promise<*>}
   */
  async dump(dtoIn, requirePrivilegedMode = true) {
    //HDS 1, A1
    let runtimeMode = await SysAppRuntimeModeModel.getAppRuntimeMode(Config.get("asid"));
    let isInPrivilegedMode = runtimeMode.mode === PRIVILEGED_MODE;
    if (!isInPrivilegedMode && requirePrivilegedMode) {
      throw new Errors.SysDump.AppNotInPrivilegedMode({}, { currentRuntimeMode: runtimeMode.mode });
    }

    //HDS 2, A2, A3
    let validationResult = this.validator.validate("sysDumpDtoInType", dtoIn);
    let uuAppErrorMap = ValidationHelper.processValidationResult(
      dtoIn,
      validationResult,
      `${Errors.SysDump.UC_CODE}unsupportedKeys`,
      Errors.SysDump.InvalidDtoInError
    );

    //HDS 3
    let awidList = dtoIn.awidList || [];

    //HDS 3.1
    let stats = await SysDataStoreStatsModel.listDataStoreStats({ awids: awidList });

    //HDS 3.2
    let totalDumpSize = 0;
    let deprecatedStats = false;
    stats.itemList.forEach(item => {
      //HDS 3.2.1
      totalDumpSize += item.size;
      if (new Date() - item.calcEndTs > 1000 * 3600 * 24 * 10) {
        deprecatedStats = true;
      }
    });
    if (stats.itemList.length === 0) {
      deprecatedStats = true;
    }

    //HDS 3.2.2
    if (deprecatedStats) {
      let dao = DaoFactory.getDaos()
        .values()
        .next().value;
      let dumpDao = this._getDumpDao(dao);
      totalDumpSize = await dumpDao.calculateDatabaseStats();
    }

    //HDS 3.3, A4
    if (totalDumpSize > MAXIMUM_DUMP_SIZE) {
      if (deprecatedStats) {
        throw new Errors.SysDump.MaximumSizeExceededNoStats(
          { uuAppErrorMap },
          {
            maxSize: MAXIMUM_DUMP_SIZE,
            size: totalDumpSize
          }
        );
      } else {
        throw new Errors.SysDump.MaximumSizeExceeded(
          { uuAppErrorMap },
          {
            maxSize: MAXIMUM_DUMP_SIZE,
            size: totalDumpSize
          }
        );
      }
    }

    //HDS 4
    let archive = new JSZIP();

    //HDS 5, 6
    await this._exportSchemas(awidList, archive);

    //HDS 7
    return await archive.generateNodeStream({ type: "nodebuffer", streamFiles: true, compression: "DEFLATE" });
  }

  /**
   * Restore database from dump
   * @param uri
   * @param dtoIn
   * @param requirePrivilegedMode
   * @returns {Promise<{uuAppErrorMap: {}}>}
   */
  async restore(uri, dtoIn, requirePrivilegedMode = true) {
    //HDS 1, A1
    let isAwidRestoreCall = !requirePrivilegedMode;
    let runtimeMode = await SysAppRuntimeModeModel.getAppRuntimeMode(Config.get("asid"));
    let isInPrivilegedMode = runtimeMode.mode === PRIVILEGED_MODE;
    let toDeleteFileIdList = new Map();
    if (!isInPrivilegedMode && requirePrivilegedMode) {
      throw new Errors.SysRestore.AppNotInPrivilegedMode({}, { currentRuntimeMode: runtimeMode.mode });
    }

    //HDS 2, A2, A3
    let validationResult = this.validator.validate("sysRestoreDtoInType", dtoIn);
    let uuAppErrorMap = ValidationHelper.processValidationResult(
      dtoIn,
      validationResult,
      `${Errors.SysRestore.UC_CODE}unsupportedKeys`,
      Errors.SysRestore.InvalidDtoInError
    );
    let isDroppedDatabase = false;

    //HDS 3
    await new Promise((resolve, reject) => {
      let unzipStream = pump(dtoIn.data, unzip.Parse());
      let entryPromise; // indicates whether last entry was processed so dtoOut returns after restoring all data
      let awidList = dtoIn.awidList || [];
      let isStrictMode = true;
      if (dtoIn.strictMode != null) {
        isStrictMode = dtoIn.strictMode;
      }

      //HDS 4
      unzipStream.on("entry", async entry => {
        entryPromise = new Promise(async resolve => {
          try {
            unzipStream.pause();
            let response = await this._restoreSchema(
              entry,
              awidList,
              uri,
              isStrictMode,
              isDroppedDatabase,
              uuAppErrorMap,
              toDeleteFileIdList,
              isAwidRestoreCall
            );
            isDroppedDatabase = response.isDroppedDatabase;
            toDeleteFileIdList = response.toDeleteFileIdList;
            unzipStream.resume();
            resolve();
          } catch (e) {
            unzipStream.destroy(e);
          }
        });
      });

      unzipStream.on("error", e => {
        if (e instanceof WorkspaceError) {
          reject(e);
        } else {
          reject(new Errors.SysRestore.RestoreFailed({ uuAppErrorMap }, { cause: e.message }, e));
        }
      });

      unzipStream.on("end", async () => {
        if (entryPromise) {
          await entryPromise;
        }
        resolve();
      });
    });

    //clear caches
    SysAppWorkspaceConfigModel.clearCache();
    SysAppWorkspaceModel.clearCache();
    SysAppRuntimeModeModel.clearCache();

    //HDS 5
    return { uuAppErrorMap };
  }

  /**
   * Return dump of database as a stream
   * use awid from request uri
   * @returns {Promise<*>}
   * @param awid
   */
  async awidDump(awid) {
    //HDS 1, A1
    let runtimeMode = await SysAppWorkspaceModel.getAwidRuntimeMode(awid);
    let isInPrivilegedMode = runtimeMode.mode === PRIVILEGED_MODE;
    if (!isInPrivilegedMode) {
      throw new Errors.SysAwidDump.AwidNotInPrivilegedMode({}, { currentRuntimeMode: runtimeMode.mode });
    }

    let dtoIn = {
      awidList: [awid]
    };

    //HDS 2, HDS 3, A2
    return this.dump(dtoIn, false);
  }

  /**
   * Restore database from dump
   * * use awid from request uri
   * @param uri
   * @param dtoIn
   * @returns {Promise<{uuAppErrorMap: {}}>}
   */
  async awidRestore(uri, dtoIn) {
    //HDS 1, A1
    let runtimeMode = await SysAppWorkspaceModel.getAwidRuntimeMode(uri.getAwid());
    let isInPrivilegedMode = runtimeMode.mode === PRIVILEGED_MODE;
    if (!isInPrivilegedMode) {
      throw new Errors.SysAwidRestore.AwidNotInPrivilegedMode({}, { currentRuntimeMode: runtimeMode.mode });
    }

    //HDS 2, A2, A3
    let validationResult = this.validator.validate("sysAwidRestoreDtoInType", dtoIn);
    let uuAppErrorMap = ValidationHelper.processValidationResult(
      dtoIn,
      validationResult,
      `${Errors.SysAwidRestore.UC_CODE}unsupportedKeys`,
      Errors.SysAwidRestore.InvalidDtoInError
    );

    dtoIn.awidList = [uri.getAwid()];

    //HDS 3, A4
    let response = await this.restore(uri, dtoIn, false);
    response.uuAppErrorMap = DeepMerge(uuAppErrorMap, response.uuAppErrorMap);

    //HDS 4
    return response;
  }

  /**
   * Restore schema
   * @param stream
   * @param awidList
   * @param uri
   * @param isStrictMode
   * @param isDroppedDatabase
   * @param uuAppErrorMap
   * @param toDeleteFileIdList
   * @param isAwidRestoreCall
   * @returns {Promise<{isDroppedDatabase: *, toDeleteFileIdList: *}>}
   * @private
   */
  async _restoreSchema(stream, awidList, uri, isStrictMode, isDroppedDatabase, uuAppErrorMap, toDeleteFileIdList, isAwidRestoreCall) {
    let schema = stream.path.substring(0, stream.path.lastIndexOf("."));

    if (schema === "status") {
      return isDroppedDatabase;
    }

    let metadataLine = true;
    let bulkQueue = [];
    let indexList = [];
    let metadataAwidList = {};
    let progressList = {};

    let dao;
    await this._eachLine(stream, async (line, lineNumber) => {
      let parsedLine = null;

      //HDS 4.7.1
      try {
        parsedLine = EJSON.parse(line);
      } catch (e) {
        if (metadataLine) {
          // A4
          throw new Errors.SysRestore.InvalidSchemaMetadata({ uuAppErrorMap }, { schema }, e);
        } else if (isStrictMode) {
          // A9.1
          throw new Errors.SysRestore.InvalidDump({ uuAppErrorMap }, { schema, lineNumber, cause: e.message }, e);
        } else {
          // A9.2
          logger.error(
            `Dump row is not valid, strict mode set to false -> continuing the restore process. Schema: ${schema}, lineNuber: ${lineNumber}.`,
            e
          );

          let invalidRowErrorCode;
          if (isAwidRestoreCall) {
            invalidRowErrorCode = new Errors.SysAwidRestore.InvalidDumpRow().code;
          } else {
            invalidRowErrorCode = new Errors.SysRestore.InvalidDumpRow().code;
          }

          if (!uuAppErrorMap[invalidRowErrorCode]) {
            ValidationHelper.addWarning(
              uuAppErrorMap,
              invalidRowErrorCode,
              "Dump row is not valid. See the cause for more details.",
              {
                rows: [
                  {
                    schema,
                    lineNumber,
                    cause: e.message
                  }
                ]
              }
            );
          } else {
            uuAppErrorMap[invalidRowErrorCode].paramMap.rows.push({ schema, cause: e.message });
          }
        }
      }
      if (parsedLine !== null) {
        if (metadataLine) {
          //HDS 4.1, 4.2, 4.3, 4.4
          parsedLine = JSON.parse(line);
          await this._validateMetadata(parsedLine, awidList, uri, schema);
          schema = parsedLine.schemaName;

          ///HDS 4.5
          dao = await this._getRestoreDao(schema, uuAppErrorMap);

          //HDS 4.6
          if (!isDroppedDatabase) {
            let response = await this._deleteAwidDocuments(awidList, parsedLine.awidList, dao, toDeleteFileIdList);
            isDroppedDatabase = response.isOnlyAsid;
            response.newDeletedFileIdList.forEach(fileId => {
              toDeleteFileIdList.set(fileId, true);
            });
          }

          parsedLine.awidList.forEach(awidObject => {
            if (awidObject.awid) {
              metadataAwidList[awidObject.awid] = awidObject;
            } else {
              metadataAwidList[awidObject.asid] = awidObject;
            }
          });

          progressList = this._createProgressList(metadataAwidList, schema);
          indexList = parsedLine.indexList;

          metadataLine = false;
        } else {
          //HDS 4.7.2
          let written = await this._processDataLine(parsedLine, toDeleteFileIdList, metadataAwidList, bulkQueue, awidList, dao, isStrictMode, progressList, schema, uuAppErrorMap);
          if (written) {
            bulkQueue = [];
          }
        }
      }
    });

    if (dao) {
      //HDS 4.8
      await this._bulkWrite(dao, bulkQueue, isStrictMode, schema, progressList, toDeleteFileIdList, uuAppErrorMap, true);
      bulkQueue = [];

      //HDS 4.9, A11
      let dumpDao = this._getDumpDao(dao, schema);
      let isAsid = awidList.length === 1 && awidList[0] === Config.get("asid");
      await dumpDao.createIndexes(indexList, isAsid, schema, uuAppErrorMap);
    }

    return { isDroppedDatabase, toDeleteFileIdList };
  }

  /**
   * Process data line
   * @param parsedLine
   * @param toDeletedFileIdList
   * @param metadataAwidList
   * @param bulkQueue
   * @param awidList
   * @param dao
   * @param isStrictMode
   * @param progressList
   * @param schema
   * @param uuAppErrorMap
   * @returns {Promise<boolean>}
   * @private
   */
  async _processDataLine(parsedLine, toDeletedFileIdList, metadataAwidList, bulkQueue, awidList, dao, isStrictMode, progressList, schema, uuAppErrorMap) {
    let checkBinaryForFileId = dao.collectionName.endsWith(".files") || dao.collectionName.endsWith(".chunks");
    let isAsid = Object.keys(metadataAwidList).length === 1 && Object.keys(metadataAwidList)[0] === Config.get("asid");

    //HDS 4.7.2
    if (checkBinaryForFileId && !isAsid) {
      let awidOfObject;
      let foundFileId = false;
      let keys = Object.keys(metadataAwidList);
      for (let i = 0; i < keys.length; i += 1) {
        let awidObject = metadataAwidList[keys[i]];
        if (awidList.length > 0 && !awidList.includes(awidObject.awid)) {
          continue;
        }
        let idToCheck;
        if (dao.collectionName.endsWith(".chunks")) {
          idToCheck = parsedLine.files_id.toString();
        } else {
          idToCheck = parsedLine._id.toString();
        }
        if (awidObject.fileIdList && awidObject.fileIdList.includes(idToCheck)) {
          foundFileId = true;
          awidOfObject = keys[i];
          break;
        }
      }
      if (foundFileId) {
        //HDS 4.7.3
        bulkQueue.push({ awid: awidOfObject, data: parsedLine });
      }
    } else if (awidList.length > 0 && awidList.includes(parsedLine.awid)) {
      //HDS 4.7.3
      bulkQueue.push({ awid: parsedLine.awid, data: parsedLine });
    } else if (awidList.length === 0) {
      //HDS 4.7.3
      bulkQueue.push({ awid: parsedLine.awid, data: parsedLine });
    }

    return await this._bulkWrite(
      dao,
      bulkQueue,
      isStrictMode,
      schema,
      progressList,
      toDeletedFileIdList,
      uuAppErrorMap
    );
  }

  /**
   * Execute bulk write to the database
   * @param dao
   * @param bulkQueue
   * @param isStrictMode
   * @param schema
   * @param progressList
   * @param toDeleteFileIdList
   * @param uuAppErrorMap
   * @param force
   * @private
   */
  async _bulkWrite(dao, bulkQueue, isStrictMode, schema, progressList, toDeleteFileIdList, uuAppErrorMap, force = false) {
    //HDS 4.7.4 A10
    if (sizeOf(bulkQueue) > QUEUE_LIMIT || force) {
      let result;
      try {
        result = await dao.insertMany(bulkQueue);
      } catch (e) {
        if (isStrictMode) {
          throw new Errors.SysRestore.RestoreFailed({ uuAppErrorMap }, { schema, cause: e.message }, e);
        } else {
          let invalidRows = [];
          let errorResult = e.result;
          let nInserted = errorResult.toJSON()["nInserted"];
          let insertedIds = errorResult.toJSON().insertedIds.slice(nInserted);
          insertedIds.forEach(insertedId => {
            invalidRows.push({ schema, objectId: insertedId._id, cause: e.message });
          });
          if (!uuAppErrorMap["rowRestoreFailed"]) {
            ValidationHelper.addWarning(
              uuAppErrorMap,
              "rowRestoreFailed",
              "Insert by DAO failed. See the cause for more details.",
              { rows: invalidRows }
            );
          } else {
            uuAppErrorMap["rowRestoreFailed"].paramMap.rows.push(...invalidRows);
          }
        }
      }
      if (result && result.insertedIds) {
        let ids = [];
        Object.keys(result.insertedIds).forEach(index => {
          let id = result.insertedIds[index];
          ids.push(id);
        });
        bulkQueue.forEach(doc => {
          if (doc.data.fileId != null && ids.concat(doc.data._id.toString())) {
            toDeleteFileIdList.set(doc.data.fileId.toString(), false);
          }
        });
      }
      //HDS 4.7.5
      this._processProgress(bulkQueue, progressList);
      return true;
    } else {
      return false;
    }
  }

  /**
   * Create progress list with count of items and count of already processed items
   * @param metadataAwidList
   * @param schema
   * @private
   */
  _createProgressList(metadataAwidList, schema) {
    let progressList = {};
    Object.keys(metadataAwidList).forEach(key => {
      let awidObject = metadataAwidList[key];
      if (awidObject.awid) {
        progressList[awidObject.awid] = { itemCount: awidObject.itemCount, processed: 0, isAsid: false };
        progressList[awidObject.awid].schema = schema;
      } else if (awidObject.asid) {
        progressList[awidObject.asid] = { itemCount: awidObject.itemCount, processed: 0, isAsid: true };
        progressList[awidObject.asid].schema = schema;
      }
    });
    return progressList;
  }

  /**
   * Log progress of each awids
   * @param bulkQueue
   * @param progressList
   * @private
   */
  _processProgress(bulkQueue, progressList) {
    if (bulkQueue.length > 0) {
      bulkQueue.forEach(document => {
        if (document.awid) {
          if (!progressList[document.awid]) {
            //only asid in progressList
            progressList[Object.keys(progressList)[0]].processed += 1;
          } else {
            progressList[document.awid].processed += 1;
          }
        }
      });

      Object.keys(progressList).forEach(key => {
        let itemCount = progressList[key]["itemCount"].value;
        let processed = progressList[key]["processed"];
        let schema = progressList[key].schema;

        let awidOrAsid = progressList[key].isAsid ? "asid" : "awid";

        if (processed === itemCount) {
          logger.info(`${processed} of ${itemCount} was processed of ${awidOrAsid} ${key} for schema: ${schema}`);
        } else {
          for (let i = 1; i <= 4; i += 1) {
            if (processed / itemCount > i * 0.195 && processed / itemCount < i * 0.201) {
              logger.info(`${processed} of ${itemCount} was processed of ${awidOrAsid} ${key} for schema: ${schema}`);
            }
          }
        }
      });
    }
  }

  /**
   * Validate metadata line
   * @param metadata
   * @param awidList
   * @param uri
   * @param schemaFilename
   * @returns {Promise<void>}
   * @private
   */
  async _validateMetadata(metadata, awidList, uri, schemaFilename) {
    //HDS 4.1 A4
    let validationResult = this.validator.validate("uuAppDataStoreDumpSchemaMetadataType", metadata);
    try {
      ValidationHelper.processValidationResult(
        metadata,
        validationResult,
        `${Errors.SysRestore.UC_CODE}unsupportedKeys`,
        Errors.SysRestore.InvalidSchemaMetadata
      );
    } catch (e) {
      e.paramMap.schema = metadata.schemaName || schemaFilename;
      throw e;
    }
    //HDS 4.2, 4.3
    for (let awid of awidList) {
      //A5
      let found = false;
      metadata.awidList.forEach(list => {
        if (list.awid === awid) {
          found = true;
        }
      });
      if (!found) {
        throw new Errors.SysRestore.AwidNotFoundInDump({}, { awid, dumpAwidList: metadata.awidList.map(o => o.awid) });
      }

      //A6
      let sysAppWorkspace;
      try {
        sysAppWorkspace = await SysAppWorkspaceModel.getAppWorkspace(uri, { awid });
      } catch (e) {
        // there is not this workspace
      }
      if (sysAppWorkspace && sysAppWorkspace.artifactUri) {
        throw new Errors.SysRestore.artifactConnectedToRestoredAwid(
          {},
          { awid, artifactUri: sysAppWorkspace.artifactUri }
        );
      }
    }

    //HDS 4.4 A7
    if (
      awidList.length === 0 &&
      metadata.awidList[0] &&
      metadata.awidList[0].asid &&
      metadata.awidList[0].asid !== Config.get("asid")
    ) {
      throw new Errors.SysRestore.AsidNotFoundInDump(
        {},
        { asid: Config.get("asid"), dumpAwidList: metadata.awidList.map(o => o.awid) }
      );
    }
  }

  /**
   * Return dao for restore process
   * @param schema
   * @param uuAppErrorMap
   * @returns {Promise<DataStoreDumpDao>}
   * @private
   */
  async _getRestoreDao(schema, uuAppErrorMap) {
    let dao;
    try {
      dao = DaoFactory.getDao(schema);
      dao.isBinary = Object.getPrototypeOf(dao.constructor).name === "UuBinaryDao";
    } catch (e) {
      let binarySchema = schema.match(/(.+)\.(chunks|files)$/);
      if (binarySchema) {
        dao = await this._getRestoreDao(binarySchema[1]);
      } else {
        //A8
        throw new Errors.SysRestore.DaoNotFound({ uuAppErrorMap }, { schema });
      }
    }
    return this._getDumpDao(dao, schema);
  }

  /**
   * Iterate whole each line
   * @param stream
   * @param cb
   * @returns {Promise<any>}
   * @private
   */
  async _eachLine(stream, cb) {
    let rl = new LineByLineReader(stream);
    let lineNumber = 0;

    return new Promise((resolve, reject) => {
      rl.on("line", async line => {
        try {
          rl.pause();
          await cb(line, lineNumber++);
          rl.resume();
        } catch (e) {
          reject(e);
          rl.close(e);
        }
      });

      rl.on("end", () => {
        resolve();
      });
    });
  }

  /**
   * Delete documents for specified awids or whole database
   * @param awidList
   * @param metadataAwidList
   * @param dao
   * @param toDeletedFileIdList
   * @returns {Promise<{isOnlyAsid: boolean, deletedFileIdList: *}>}
   * @private
   */
  async _deleteAwidDocuments(awidList, metadataAwidList, dao, toDeletedFileIdList) {
    //HDS 4.5.1
    let isOnlyAsid = false;
    let isAsidInInputAwidList = awidList.length === 1 && awidList[0] === Config.get("asid");
    let isAsidInMetadataAwidList = metadataAwidList.length === 1 && metadataAwidList[0].asid != null;

    //HDS 4.5.2
    if (isAsidInMetadataAwidList || isAsidInInputAwidList) {
      isOnlyAsid = true;
    }

    let newDeletedFileIdList = [];
    if (isOnlyAsid) {
      await SysDumpRestoreMongo.dropAllDatabases();
    } else {
      let dumpDao = this._getDumpDao(dao);
      let filter;
      if (awidList && awidList.length > 0) {
        filter = { awid: { $in: awidList } };
      } else {
        filter = { awid: { $in: metadataAwidList.map(obj => obj.awid) } };
      }

      //HDS 4.5.3
      if (!dao.isBinary) {
        //HDS 4.5.3.1
        if (dao.collectionName.endsWith(".files") || dao.collectionName.endsWith(".chunks")) {
          let schema = dao.collectionName.substring(0, dao.collectionName.lastIndexOf("."));
          newDeletedFileIdList = await dumpDao.deleteBinary(filter, schema, toDeletedFileIdList);
        } else {
          await dumpDao.deleteMany(filter);
        }
      } else {
        //HDS 4.5.3.2
        newDeletedFileIdList = await dumpDao.deleteBinary(filter, dao.collectionName, toDeletedFileIdList);
      }
    }

    return { isOnlyAsid, newDeletedFileIdList };
  }

  /**
   * Fill stream with exports of schemas converted to JSON.
   * @param awidList
   * @param archive
   * @return {Promise<void>}
   * @private
   */
  async _exportSchemas(awidList, archive) {
    let startTime = new Date().toISOString();

    //HDS 6.1
    try {
      for (let dao of DaoFactory.getDaos().values()) {
        let dumpDao = this._getDumpDao(dao);

        //HDS 6.2
        let searchQuery = this._createSearchQuery(dumpDao, awidList);
        await this._saveSchema(searchQuery, dumpDao, archive, awidList);
      }

      this._saveStatus(archive, "finished", startTime);
    } catch (e) {
      logger.error("Export failed.", e);
      this._saveStatus(archive, "error", startTime, e);
    }
  }

  /**
   * Create search query for database
   * @param dumpDao
   * @param awidList
   * @returns {*}
   * @private
   */
  _createSearchQuery(dumpDao, awidList) {
    if (!awidList || (Array.isArray(awidList) && awidList.length === 0)) {
      return {};
    }

    const filter = Array.isArray(awidList) ? { $in: awidList } : awidList;
    if (dumpDao.isAsid) {
      return { asid: filter };
    } else {
      return { awid: filter };
    }
  }

  /**
   * Add all documents of DAO to stream. If Dao is binary, add also its data (.files and .chunks)
   * @param filter
   * @param dao
   * @param archive
   * @param awidList
   * @return {Promise<void>}
   * @private
   */
  async _saveSchema(filter, dao, archive, awidList) {
    const schema = dao.collectionName;
    const binaryFilesIds = [];

    const stringifyStream = new Transform({
      writableObjectMode: true,
      transform(doc, opts, callback) {
        let chunk;
        if (opts && opts.metadata === true) {
          chunk = `${JSON.stringify(doc)}\n`;
        } else {
          if (schema.endsWith(".chunks") && doc.data && doc.data._bsontype === "Binary") {
            doc.data = new Binary(doc.data.buffer, doc.data.sub_type);
          }
          chunk = `${EJSON.stringify(doc)}\n`;
          dao.isBinary && binaryFilesIds.push(doc.fileId);
        }
        this.push(chunk);
        callback();
      }
    });

    let metadata = await this._loadMetadata(awidList, dao, schema);

    stringifyStream.write(metadata, { metadata: true });

    let documents = await dao.find(filter);
    pump(documents, stringifyStream);

    archive.file(`${schema}.json`, stringifyStream);

    if (dao.isBinary) {
      let filesDao = this._getDumpDao(dao, `${schema}.files`);
      await this._saveSchema({ _id: { $in: binaryFilesIds } }, filesDao, archive, awidList);

      let chunksDao = this._getDumpDao(dao, `${schema}.chunks`);
      await this._saveSchema({ files_id: { $in: binaryFilesIds } }, chunksDao, archive, awidList);
    }
  }

  /**
   * Load metadata
   * @param awidList
   * @param dao
   * @param schema
   * @returns {Promise<{schemaName: *, awidList: Array, cts: string, uuSubAppVersion: string}>}
   * @private
   */
  async _loadMetadata(awidList, dao, schema) {
    let awidListWithCount = [];
    let isAsid = false;
    let asid;
    let list = [];

    if (awidList.length === 0) {
      asid = Config.get("asid");
      list.push(asid);
      isAsid = true;
    } else {
      list = awidList;
    }

    let promises = list.map(async awid => {
      let resultObject;
      let awidObject = {};

      if (isAsid) {
        resultObject = await this._loadCountsAndFileIdList(dao);
        awidObject.asid = asid;
      } else {
        resultObject = await this._loadCountsAndFileIdList(dao, { awid });
        awidObject.awid = awid;
      }

      awidObject.itemCount = resultObject.itemCount;

      let shouldHaveFileIdList = dao.collectionName.endsWith(".files") || dao.collectionName.endsWith(".chunks");
      if (shouldHaveFileIdList && !isAsid) {
        awidObject.fileIdList = resultObject.fileIdList;
      }

      awidListWithCount.push(awidObject);
    });

    await Promise.all(promises);

    let metadata = {
      schemaName: schema,
      awidList: awidListWithCount,
      cts: new Date().toISOString(),
      uuSubAppVersion: Sys.getAppInfo().uuSubAppVersion
    };

    try {
      let indexes = await dao.getIndexes();
      metadata.indexList = await indexes.toArray();
    } catch (e) {
      if (e.code !== 26) {
        // 26 ~ collection not initialized
        throw new BaseError("Error during export of indexes, collection: " + schema, e);
      }
      metadata.indexList = [];
    }

    return metadata;
  }

  /**
   * Load counts and fileIdList for specif schema
   * @param dao
   * @param filter
   * @returns {Promise<{itemCount: (*|number), fileIdList: Array, awid: (awid|{$in})}>}
   * @private
   */
  async _loadCountsAndFileIdList(dao, filter = {}) {
    let fileIdList = [];
    let itemCount = 0;

    if (dao.collectionName.endsWith(".files") || dao.collectionName.endsWith(".chunks")) {
      let files = await dao.getFileIdList(filter);
      files.forEach(file => {
        fileIdList.push(file.fileId);
      });
      if (dao.collectionName.endsWith(".files")) {
        itemCount = fileIdList.length;
      } else {
        itemCount = await dao.binaryCount(fileIdList);
      }
    } else {
      itemCount = await dao.count(filter);
    }

    return {
      itemCount,
      fileIdList,
      awid: filter.awid
    };
  }

  /**
   * Create status.json file
   * @param archive
   * @param status
   * @param startTime
   * @param uuAppErrorMap
   * @private
   */
  _saveStatus(archive, status, startTime, uuAppErrorMap = {}) {
    let dumpStatus = {
      status,
      startTime,
      uuAppErrorMap,
      endTime: new Date().toISOString()
    };

    archive.file(STATUS_FILE, JSON.stringify(dumpStatus));
  }

  /**
   * Return DumpDao
   * @param dao
   * @param name
   * @returns {DataStoreDumpDao}
   * @private
   */
  _getDumpDao(dao, name) {
    let collectionName = name || dao.collectionName;
    let dumpDao = new DataStoreDumpDao(collectionName, null, null, dao.customUri);
    dumpDao.isBinary = Object.getPrototypeOf(dao.constructor).name === "UuBinaryDao";
    dumpDao.isAsid = dao.getExtraAttributes().asid === true;

    return dumpDao;
  }

  async _importSchema(entry, uuAppErrorMap, newAwid, appWorkspace, deletedBinaryRegister, fileIdMapping) {

    if (entry.path === STATUS_FILE) {
      return;
    }

    let metadata, dumpDao;
    let lastLoggedAt = 0, restored = 0;
    let documentQueue = [], mappingsQueue = [];

    await this._eachLine(entry, async (line, lineNumber) => {
      if (lineNumber === 0) { // hds 5.1
        // hds 5.1.1
        try {
          metadata = JSON.parse(line);
        } catch (e) { // A5
          throw new Errors.SysImport.InvalidSchemaMetadata({ uuAppErrorMap }, { cause: e }, e);
        }
        let validationResult = this.validator.validate("sysImportSchemaMetadataType", metadata);
        if (!validationResult.isValid()) { // A5
          throw new Errors.SysImport.InvalidSchemaMetadata({ uuAppErrorMap }, { schema: metadata.schemaName, ...validationResult.getValidationErrorMap() });
        }
        metadata = ImportMetadata.parse(metadata);

        if (metadata.getSchema() === IMPORT_MAPPING_SCHEMA) {
          return;
        }

        // hds 5.1.2
        let daoSchemaName = metadata.getBinarySchema() || metadata.getSchema();
        let dao;
        try {
          dao = DaoFactory.getDao(daoSchemaName);
        } catch (e) {
          // A6
          throw new Errors.SysImport.DaoNotFound({ uuAppErrorMap }, { schema: metadata.getSchema() });
        }
        dumpDao = this._getDumpDao(dao, metadata.getSchema());
        metadata.setBinary(dumpDao.isBinary);

        // hds 5.1.3
        if (!metadata.isBinary()) {
          // hds 5.1.3.1
          logger.debug(`Deleting records of uuObject schema ${metadata.getSchema()} for awid ${newAwid}`);
          await dumpDao.deleteManyByAwid(newAwid);
        } else if (!deletedBinaryRegister.includes(daoSchemaName)) {
          // hds 5.1.3.2
          logger.debug(`Deleting records of uuBinary schema ${metadata.getSchema()} for awid ${newAwid}`);
          await dumpDao.deleteBinary({ awid: newAwid }, daoSchemaName);
          deletedBinaryRegister.push(daoSchemaName);
        } else {
          logger.debug(`Records of uuBinary schema ${metadata.getSchema()} already deleted for awid ${newAwid}`);
        }

      } else { // hds 5.2
        if (metadata.getSchema() === IMPORT_MAPPING_SCHEMA) {
          return;
        }

        // 5.2.1
        let document;
        try {
          document = EJSON.parse(line);
        } catch (e) {
          throw new Errors.SysImport.InvalidDump({ uuAppErrorMap }, { schema: metadata.getSchema(), cause: e }, e);
        }

        // hds 5.2.2
        if (!metadata.isBinaryAux()) {
          document.awid = newAwid;
        }

        // hds 5.2.3
        let newId, oldId;
        // _id of *.files could have been already recreated, so new _id !!CANNOT!! be created again
        if (metadata.isBinaryFiles() && fileIdMapping[metadata.getBinarySchema()] && fileIdMapping[metadata.getBinarySchema()][document._id]) {
          newId = fileIdMapping[metadata.getBinarySchema()][document._id];
          oldId = document._id;
          document._id = newId;
        } else {
          newId = this.importMappingDao.getObjectId();
          oldId = document._id;
          document._id = newId;
          // this is technically part of hds 5.2.5, but it has to be here
          mappingsQueue.push({ origId: oldId, newId: newId, origAwid: metadata.getAwid(), awid: newAwid });
        }

        // hds 5.2.3.1
        if (metadata.isBinary()) {
          let newIdMapping = this._replaceFileId(metadata, document, fileIdMapping, newId, oldId);
          if (newIdMapping) {
            mappingsQueue.push({ origId: newIdMapping.orig, newId: newIdMapping.new, origAwid: metadata.getAwid(), awid: newAwid });
          }
        }

        // hds 5.2.4
        if (metadata.getSchema() === "sysAppWorkspace") {
          this._handleAppWorkspace(document, appWorkspace, uuAppErrorMap);
        }

        // hds 5.2.5
        documentQueue.push({ data: document });

        // hds 5.2.6., 5.2.7, 5.2.8, A9, A10
        if (sizeOf(documentQueue) > QUEUE_LIMIT) {
          ({ restored, lastLoggedAt } = await this._saveAndLogInsert(documentQueue, mappingsQueue, uuAppErrorMap, metadata, dumpDao, restored, lastLoggedAt, newAwid));
        }
      }
    });

    if (metadata.getSchema() === IMPORT_MAPPING_SCHEMA) {
      return;
    }

    // hds 5.3, hds 5.3.1, hds 5.3.2, A11, A12
    if (documentQueue.length !== 0 || mappingsQueue.length !== 0) {
      await this._saveAndLogInsert(documentQueue, mappingsQueue, uuAppErrorMap, metadata, dumpDao, restored, lastLoggedAt, newAwid, true);
    }

    // hds 5.4
    let indexFailures = await dumpDao.importIndexes(metadata.getIndexList());
    if (indexFailures.length !== 0) {
      // A13
      if (!uuAppErrorMap[WARNINGS.importIndexImportFailed]) {
        let firstFailure = indexFailures.shift();
        ValidationHelper.addWarning(
          uuAppErrorMap,
          WARNINGS.importIndexImportFailed,
          "Create index by DAO failed.",
          {
            rows: [{
              schema: metadata.getSchema(),
              index: firstFailure.index,
              cause: firstFailure.cause
            }]
          }
        );
      }
      for (let failure of indexFailures) {
        uuAppErrorMap[WARNINGS.importIndexImportFailed].paramMap.rows.push({ schema: metadata.getSchema(), index: failure.index, cause: failure.cause });
      }
    }
  }

  async _saveAndLogInsert(documentQueue, mappingsQueue, uuAppErrorMap, metadata, dumpDao, restored, lastLoggedAt, awid, force = false) {
    if (documentQueue.length > 0) {
      try {
        await dumpDao.insertMany(documentQueue);
      } catch (e) { // A9
        throw new Errors.SysImport.ImportFailed({ uuAppErrorMap }, { schema: metadata.getSchema(), cause: e }, e);
      }
    }

    if (mappingsQueue.length > 0) {
      try {
        await this.importMappingDao.create(mappingsQueue);
      } catch (e) { // A10
        if (e instanceof ObjectStoreError) {
          throw new Errors.SysImport.SysImportMappingDaoCreateFailed({ uuAppErrorMap }, e);
        }
        throw e;
      }
    }

    restored += documentQueue.length;
    if (force || restored - lastLoggedAt > metadata.getRecordCount() + 0.2) {
      logger.info(`Schema: ${metadata.getSchema()}, awid: ${awid}, imported ${restored} out of ${metadata.getRecordCount()} entries`);
      lastLoggedAt = restored;
    }

    // this clears the queues
    documentQueue.length = 0;
    mappingsQueue.length = 0;

    return { restored, lastLoggedAt };
  };

  _replaceFileId(metadata, document, fileIdMapping, newId, oldId) {
    // record filled when new fileId was created, i.e. only when generating new ObjectId,
    // which is a foreign key (happen when either chunks or binary itself are processed
    // before files are
    let newIdMapping = null;

    // get the value of fileId, which is about to be replaced
    let origFileId;
    if (metadata.isBinaryFiles()) {
      origFileId = oldId;
    } else if (metadata.isBinaryChunks()) {
      origFileId = document.files_id;
    } else { // the uuBinary
      origFileId = document.fileId;
    }

    // check the registry of fileIds mapping
    let key = metadata.getBinarySchema() || metadata.getSchema();
    let newFileId;

    // the registry for this schema was not created yet
    if (!fileIdMapping[key]) {
      fileIdMapping[key] = {};
    }

    if (!fileIdMapping[key][origFileId]) {
      // it is not in the registry (i.e. it was not replaced for either of binary schemas)
      if (metadata.isBinaryFiles()) {
        // fileId is _id for *.files schema => ObjectId was already generated in hds 5.2.3
        newFileId = newId;
      } else {
        // fileId is a foreign key here => new ObjectId is needed
        newFileId = this.importMappingDao.getObjectId();
        newIdMapping = { new: newFileId, orig: origFileId };
      }
      fileIdMapping[key][origFileId] = newFileId;
    } else {
      // fileId is in the registry, i.e. one of the collections already replaced all of the fileIds
      newFileId = fileIdMapping[key][origFileId];
    }

    // set the appropriate fileId
    if (metadata.isBinaryFiles()) {
      document._id = newFileId;
    } else if (metadata.isBinaryChunks()) {
      document.files_id = newFileId;
    } else { // the uuBinary
      document.fileId = newFileId;
    }
    return newIdMapping;
  }

  _handleAppWorkspace(document, appWorkspace, uuAppErrorMap) {
    // hds 5.2.4.1
    document.awidOwner = appWorkspace.awidOwner;
    // hds 5.2.4.2
    document.runtimeMode = appWorkspace.runtimeMode;
    // hds 5.2.4.3
    let profilesRecord = document.profileList ? document.profileList.sort() : [];
    let recordAppWorkspace = appWorkspace.profileList.sort();
    if (profilesRecord.length !== recordAppWorkspace.length || !profilesRecord.every((elem, idx) => (elem === recordAppWorkspace[idx]))) {
      ValidationHelper.addWarning(
        uuAppErrorMap,
        WARNINGS.importConflictingProfiles,
        "The profileList of original sysAppWorkspace differs from profileList of imported sysAppWorkspace.",
        {
          originalProfileList: appWorkspace.profileList,
          importedProfileList: document.profileList || []
        }
      );
    }
  }

}

module.exports = new SysDumpRestoreModel();
